CONSISTENCY MODELS

In a distributed system it's extremely hard to support storage consistency, 
but it's possible to support client consistency.

There're 4 flavors of client consistency:

1) Monotonic read consistency: if process reads a value of X, next reads will
                               return that value, or later value.
2) Monotonic write: every write operation to X happens before every next write
                    operations to X.
3) Your-writes consistency: write operation to X always visible to further
                            reads from X.
4) Write-follow-reads consistency: write next to the read of X always happens
                                   with just read or later value of X.
                                   
It should be possible to support all 4 models. To accomplish this, we have to 
manipulate with versions, diffs and timestamps.




LOG FILE (not finished)

We have 4 major mutation types:
1) Creation
2) Deletion
3) Update (dumb overwrite)
4) Custom

Example:

[ lamport_timestamp, "create", uuid, raw_doc ]  # [10, "create", "12345678...", {'__meta__'=> ...}]
[ lamport_timestamp, "delete", uuid ]
[ lamport_timestamp, "update", uuid, raw_doc ]
(or [ lamport_timestamp, "update", uuid, version_from, raw_diff ]) # TODO: find out what's easier
[ lamport_timestamp, "custom", raw_mutation_doc]

Mutation doc is a doc specifing a transactional mutation. 
E.g. "debit 1% to an account". Operation +1% could be implemented as 
an overwriting update (i.e. "take value", "increment value", 
"store new value"), but it is not merged well in case of concurrent
operations on the same data in a separate storage. To fix this, we 
implement Mutation Docs. 
Meta of the doc specifies type of mutation and its algorithms. 
Doc's slots specify parameters of the mutation.

TODO: specify log merging and rerunning 


SKIPLIST

For the best search speed, probability should be 1/E 
(1/2.718281828459045 == 0.367879441171442)

For effectiveness and distribution, we cut skiplists into 
contiguous chunks. Each chunk is identified by the first item.
To split list into chunks we define "cut level": chunks are
produced by cutting a list by every node which level is equal or
greater, than cut level. 

Example:

cut level = 3

     4  o-------------------------o-...
     3  o---------o-------o-------o-...
     2  o---o-----o---o---o-----o-o-...
     1  o-o-o-o-o-o-o-o-o-o-o-o-o-o-...
uuid    A B C D E F G H I J K L M T
level   4 1 2 1 1 3 1 2 1 3 1 1 2 4
cut     ^         ^       ^       ^

                  ||
                  \/

     4  o--------->T                          o-...
     3  o--------->F  o------->J  o------->T  o-...
     2  o---o----->F  o---o--->J  o-----o->T  o-...
     1  o-o-o-o-o->F  o-o-o-o->J  o-o-o-o->T  o-...
uuid    A B C D E     F G H I     J K L M     T
level   4 1 2 1 1     3 1 2 1     3 1 1 2     4
        Chunk A.      Chunk F.    Chunk J.    Chunk T.



For a sublist of length N, and cut level L the following is true:

         exp(L-1) = N;    L = log(N) + 1

If'd like N=1000 items per file, cut level'd be L=8.

N      L
--------
32     4
64     5
128    6
256    7
512    7
1024   8
2048   9
4096   9
8192  10
